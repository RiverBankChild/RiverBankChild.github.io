---
layout: default
title: Home
---

### Hi there, I'm Jianghan Zhou ðŸ‘‹

**AI Algorithm Engineer @ Kunlun Digital Technology | Independent Researcher**

> *"Tracing the isomorphism between neural activations and systemic intelligence."*

---

### ðŸ§ About Me

I act as a bridge between **industrial AI engineering** and **theoretical abstraction**.

Currently, I work as an Algorithm Engineer at **Kunlun Digital Technology (æ˜†ä»‘æ•°æ™º)**, focusing on the practical deployment of LLMs, RAG, and Agentic systems in complex enterprise environments.

**My Research Philosophy:**
I define myself as an **"Architectural Essentialist."** While I value rigorous mathematical proof, I believe that **structural intuition** precedes it. My research stems from observing patterns in engineering practiceâ€”identifying how mechanisms at the micro-scale (neurons) echo those at the macro-scale (agents).

I am dedicated to finding a **unified theoretical framework** that connects isolated AI concepts (Activations, Attention, Routing) into a continuous spectrum of information processing.

---

### ðŸ”¬ Featured Research

#### ðŸ“„ *Attention as a Generalized Activation Mechanism: A Hierarchical View*
> *Draft Available / Under Submission*

This paper challenges the traditional separation between Activation Functions and Attention Mechanisms. By introducing the **"Hierarchical Attention Framework,"** I demonstrate that they are structurally identical operations occurring at different dimensions of abstraction.

**Key Contribution: The 4-Dimensional Ladder**
*   **D1 (Neuronal):** Existence Gating (e.g., ReLU) â€” *Scalar level*
*   **D2 (Contextual):** Relevance Weighting (Self-Attention) â€” *Vector level*
*   **D3 (Functional):** Expert Routing (MoE) â€” *Function level*
*   **D4 (Systemic):** Systemic Attention (Multi-Agent Supervisor) â€” *Agent level*

**Core Insight:**
The paper theoretically argues (and empirically validates) that **Multi-Agent Systems (D4) are mathematically unstable without a centralized "Supervisor" (Systemic Attention)**. Just as a neural network collapses into linearity without activation functions, a MAS collapses into entropy without systemic gating.

---

### ðŸ”­ Research Interests & Vision

My long-term goal is to explore the **"First Principles" of Intelligence**.

I am currently investigating:
*   **The Continuum of Gating:** How information filtering scales from biological synapses to digital agents.
*   **Entropy in Agents:** Modeling the error propagation in autonomous systems using information theory.
*   **Unified Architectures:** Moving beyond the "Transformer + Tool" paradigm towards recursive, self-similar cognitive architectures.

*I welcome discussions on the theoretical foundations of AGI, particularly regarding the structural isomorphisms in intelligent systems.*

---

### ðŸ›  Tech Stack

*   **Theory:** System Architecture, Information Theory, First Principles Thinking.
*   **Engineering:** Python, PyTorch, LLM Inference Optimization (vLLM/DeepSpeed), Multi-Agent Orchestration (LangGraph).

---

### ðŸ“« Contact

*   **Email:** riverbankchild@gmail.com
*   **LinkedIn:** https://www.linkedin.com/in/jianghan-zhou-b4b88539b/

<!--
If you have a PDF of your first paper, link it here:
[Download Paper 1 Draft](./papers/hierarchical_attention_draft.pdf)
-->
